{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n        print(os.path.join(dirname))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T18:14:32.463106Z","iopub.execute_input":"2022-05-23T18:14:32.463430Z","iopub.status.idle":"2022-05-23T18:14:46.276518Z","shell.execute_reply.started":"2022-05-23T18:14:32.463393Z","shell.execute_reply":"2022-05-23T18:14:46.275534Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"traindir = '../input/chest-xray-pneumonia/chest_xray/train'\ntestdir = '../input/chest-xray-pneumonia/chest_xray/test'\nvaldir = '../input/chest-xray-pneumonia/chest_xray/val'","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:46.278671Z","iopub.execute_input":"2022-05-23T18:14:46.278978Z","iopub.status.idle":"2022-05-23T18:14:46.284718Z","shell.execute_reply.started":"2022-05-23T18:14:46.278939Z","shell.execute_reply":"2022-05-23T18:14:46.283742Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nimport optuna\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:46.286385Z","iopub.execute_input":"2022-05-23T18:14:46.287175Z","iopub.status.idle":"2022-05-23T18:14:49.884893Z","shell.execute_reply.started":"2022-05-23T18:14:46.287130Z","shell.execute_reply":"2022-05-23T18:14:49.884031Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import glob\n\ntrain_p = glob.glob(traindir+'/PNEUMONIA/*jpeg')\ntrain_n = glob.glob(traindir+'/NORMAL/*jpeg')\n\ndata = pd.DataFrame(np.concatenate([[0]*len(train_n), [1]*len(train_p)]), columns=[\"class\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:49.888433Z","iopub.execute_input":"2022-05-23T18:14:49.888683Z","iopub.status.idle":"2022-05-23T18:14:49.919276Z","shell.execute_reply.started":"2022-05-23T18:14:49.888654Z","shell.execute_reply":"2022-05-23T18:14:49.918502Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Transforming Images \n- Resizing \n- Normalizing \n- Applying Random Rotations","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\ntrain_trnsf = transforms.Compose([transforms.RandomRotation((-20,20)),\n                                 transforms.Resize((224,224)),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n\ntest_trnsf = transforms.Compose([transforms.Resize((224,224)),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:49.920537Z","iopub.execute_input":"2022-05-23T18:14:49.920824Z","iopub.status.idle":"2022-05-23T18:14:50.159706Z","shell.execute_reply.started":"2022-05-23T18:14:49.920787Z","shell.execute_reply":"2022-05-23T18:14:50.158815Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder \ntrain_ds = ImageFolder(traindir, train_trnsf)\ntest_ds = ImageFolder(testdir, test_trnsf)\nval_ds = ImageFolder(valdir, test_trnsf)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:50.161516Z","iopub.execute_input":"2022-05-23T18:14:50.161829Z","iopub.status.idle":"2022-05-23T18:14:50.985510Z","shell.execute_reply.started":"2022-05-23T18:14:50.161787Z","shell.execute_reply":"2022-05-23T18:14:50.984646Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset = []\ndataset = torch.utils.data.ConcatDataset([dataset, train_ds])\n# dataset = torch.utils.data.ConcatDataset([dataset, test_ds])\n# dataset = torch.utils.data.ConcatDataset([dataset, val_ds])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:50.987180Z","iopub.execute_input":"2022-05-23T18:14:50.987487Z","iopub.status.idle":"2022-05-23T18:14:50.992533Z","shell.execute_reply.started":"2022-05-23T18:14:50.987449Z","shell.execute_reply":"2022-05-23T18:14:50.991274Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 64\ntrainloader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\ntestloader = DataLoader(test_ds, batch_size, shuffle = True, num_workers=2, pin_memory=True)\nvalloader = DataLoader(val_ds, batch_size*2, shuffle = True, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:50.994638Z","iopub.execute_input":"2022-05-23T18:14:50.995265Z","iopub.status.idle":"2022-05-23T18:14:51.003960Z","shell.execute_reply.started":"2022-05-23T18:14:50.995221Z","shell.execute_reply":"2022-05-23T18:14:51.003029Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs Number of epochs')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.005940Z","iopub.execute_input":"2022-05-23T18:14:51.006395Z","iopub.status.idle":"2022-05-23T18:14:51.016218Z","shell.execute_reply.started":"2022-05-23T18:14:51.006241Z","shell.execute_reply":"2022-05-23T18:14:51.015252Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Defining the ResNet model ","metadata":{}},{"cell_type":"code","source":"class block(nn.Module):\n    def __init__(\n        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n    ):\n        super(block, self).__init__()\n        self.expansion = 4\n        self.conv1 = nn.Conv2d(\n            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n        self.conv2 = nn.Conv2d(\n            intermediate_channels,\n            intermediate_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n        self.conv3 = nn.Conv2d(\n            intermediate_channels,\n            intermediate_channels * self.expansion,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.identity_downsample = identity_downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x.clone()\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n\n        if self.identity_downsample is not None:\n            identity = self.identity_downsample(identity)\n\n        x += identity\n        x = self.relu(x)\n        return x\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, image_channels, num_classes):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Essentially the entire ResNet architecture are in these 4 lines below\n        self.layer1 = self._make_layer(\n            block, layers[0], intermediate_channels=64, stride=1\n        )\n        self.layer2 = self._make_layer(\n            block, layers[1], intermediate_channels=128, stride=2\n        )\n        self.layer3 = self._make_layer(\n            block, layers[2], intermediate_channels=256, stride=2\n        )\n        self.layer4 = self._make_layer(\n            block, layers[3], intermediate_channels=512, stride=2\n        )\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * 4, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc(x)\n\n        return x\n\n    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n        identity_downsample = None\n        layers = []\n\n        if stride != 1 or self.in_channels != intermediate_channels * 4:\n            identity_downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.in_channels,\n                    intermediate_channels * 4,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False\n                ),\n                nn.BatchNorm2d(intermediate_channels * 4),\n            )\n\n        layers.append(\n            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n        )\n\n        self.in_channels = intermediate_channels * 4\n\n\n        for i in range(num_residual_blocks - 1):\n            layers.append(block(self.in_channels, intermediate_channels))\n\n        return nn.Sequential(*layers)\n\n\n# def ResNet50(img_channel=3, num_classes=1000):\n#     return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n\n\n# def ResNet101(img_channel=3, num_classes=1000):\n#     return ResNet(block, [3, 4, 23, 3], img_channel, num_classes)\n\n\n# def ResNet152(img_channel=3, num_classes=1000):\n#     return ResNet(block, [3, 8, 36, 3], img_channel, num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.021754Z","iopub.execute_input":"2022-05-23T18:14:51.022166Z","iopub.status.idle":"2022-05-23T18:14:51.051425Z","shell.execute_reply.started":"2022-05-23T18:14:51.022047Z","shell.execute_reply":"2022-05-23T18:14:51.050362Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Function for training and evaluating models with different parameters for optuna to search for the best one ","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(param, model):\n    \n    run_epochs(param, model)\n        \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in valloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    accuracy = (100*correct/total)\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.053021Z","iopub.execute_input":"2022-05-23T18:14:51.053489Z","iopub.status.idle":"2022-05-23T18:14:51.068372Z","shell.execute_reply.started":"2022-05-23T18:14:51.053443Z","shell.execute_reply":"2022-05-23T18:14:51.067122Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Defining the objective function that tries and checks the accuracy of the model with different learning rates and optimizers for finding the best hyperparameters","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    model = ResNet(block, [2,2,2,2], 3, 2)\n    \n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n        'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n        'epochs': 2\n        #'criterion': trial.suggest_categorical(\"criterion\", [\"NLLLoss\", \"CrossEntropyLoss\", \"GaussianNLLLoss\"])\n    }   \n    \n    accuracy = train_and_evaluate(params, model)\n    \n    return accuracy\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:38:37.144727Z","iopub.execute_input":"2022-05-22T11:38:37.14553Z","iopub.status.idle":"2022-05-22T11:38:37.15185Z","shell.execute_reply.started":"2022-05-22T11:38:37.145494Z","shell.execute_reply":"2022-05-22T11:38:37.151186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Executing the optuna study function that expermiments with different hyperparameters and finds the one with the maximum accuracy","metadata":{}},{"cell_type":"code","source":"EPOCHS = 30\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nstudy.optimize(objective, n_trials=30)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:05:10.134717Z","iopub.execute_input":"2022-05-21T14:05:10.135379Z","iopub.status.idle":"2022-05-21T14:05:10.139949Z","shell.execute_reply.started":"2022-05-21T14:05:10.135333Z","shell.execute_reply":"2022-05-21T14:05:10.139112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using the best hyperparameters found by optuna","metadata":{}},{"cell_type":"code","source":"params = {\n    \"optimizer\": \"RMSprop\",\n    \"learning_rate\": 0.0003264,\n    \"epochs\": 8\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.104104Z","iopub.execute_input":"2022-05-23T18:14:51.104416Z","iopub.status.idle":"2022-05-23T18:14:51.115430Z","shell.execute_reply.started":"2022-05-23T18:14:51.104371Z","shell.execute_reply":"2022-05-23T18:14:51.114378Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Function to train the model","metadata":{}},{"cell_type":"code","source":"def run_epochs(param,model,trainloader):\n    model = model.to(device)\n    best_model = model\n    #criterion = getattr(nn, param['criterion'])()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = getattr(optim, param['optimizer'])(model.parameters(), lr=param['learning_rate'])\n    num_epochs = param['epochs']\n    \n    step = 0\n    train_losses = []\n    accuracies = []\n    best_acc = 0\n    accuracy = 0\n\n    torch.cuda.empty_cache()\n\n    for epoch in range(num_epochs):\n        running_accuracy = []\n        losses = []\n        for i, (images,labels) in enumerate(trainloader):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            losses.append(loss.item())            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            _, predictions = outputs.max(1)\n            num_correct = (predictions == labels).sum()\n            running_train_acc = float(num_correct)/float(images.shape[0])\n            running_accuracy.append(running_train_acc)\n        \n        train_losses.append(sum(losses) / len(losses))\n        accuracy = sum(running_accuracy) / len(running_accuracy)\n        accuracies.append(accuracy)\n        print(\"Accuracy for epoch {} = {}\".format(epoch+1, accuracy*100))\n                \n#             train_acc += running_train_acc\n#             train_loss += loss.item()\n#             avg_train_acc = train_acc / len(trainloader)\n#             avg_train_loss = train_loss / len(trainloader)\n\n            \n    return train_losses, accuracies, model","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.069785Z","iopub.execute_input":"2022-05-23T18:14:51.070804Z","iopub.status.idle":"2022-05-23T18:14:51.086357Z","shell.execute_reply.started":"2022-05-23T18:14:51.070721Z","shell.execute_reply":"2022-05-23T18:14:51.085215Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Function to Check the model's accuracy with other data","metadata":{}},{"cell_type":"code","source":"def check_model(model, trainloader):\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in trainloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n\n        accuracy = (100*correct/total)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.089143Z","iopub.execute_input":"2022-05-23T18:14:51.089428Z","iopub.status.idle":"2022-05-23T18:14:51.101190Z","shell.execute_reply.started":"2022-05-23T18:14:51.089394Z","shell.execute_reply":"2022-05-23T18:14:51.100042Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\nmodel = ResNet(block, [2,2,2,2], 3, 2)\n\nlosses, accuracies, model = run_epochs(params,model,trainloader)\n\nprint(\"--- %s minutes ---\" % ((time.time() - start_time)/60))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:14:51.116658Z","iopub.execute_input":"2022-05-23T18:14:51.118513Z","iopub.status.idle":"2022-05-23T18:32:35.922133Z","shell.execute_reply.started":"2022-05-23T18:14:51.118451Z","shell.execute_reply":"2022-05-23T18:32:35.920999Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the losses","metadata":{}},{"cell_type":"code","source":"def plot_losses(losses):\n    plt.plot(losses, '-bx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:32:35.924194Z","iopub.execute_input":"2022-05-23T18:32:35.924746Z","iopub.status.idle":"2022-05-23T18:32:35.930659Z","shell.execute_reply.started":"2022-05-23T18:32:35.924700Z","shell.execute_reply":"2022-05-23T18:32:35.929789Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plot_losses(losses)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:32:35.945915Z","iopub.execute_input":"2022-05-23T18:32:35.946210Z","iopub.status.idle":"2022-05-23T18:32:36.183994Z","shell.execute_reply.started":"2022-05-23T18:32:35.946178Z","shell.execute_reply":"2022-05-23T18:32:36.183206Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the accuracies","metadata":{}},{"cell_type":"code","source":"def plot_accuracies(accuracy):\n    plt.plot(accuracies,'-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs Number of Epochs')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:32:35.934082Z","iopub.execute_input":"2022-05-23T18:32:35.934959Z","iopub.status.idle":"2022-05-23T18:32:35.942345Z","shell.execute_reply.started":"2022-05-23T18:32:35.934911Z","shell.execute_reply":"2022-05-23T18:32:35.941381Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(accuracies)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:32:36.185578Z","iopub.execute_input":"2022-05-23T18:32:36.185876Z","iopub.status.idle":"2022-05-23T18:32:36.398331Z","shell.execute_reply.started":"2022-05-23T18:32:36.185845Z","shell.execute_reply":"2022-05-23T18:32:36.397510Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print('Validation accuracy = %f' % check_model(model,valloader))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:44:04.558160Z","iopub.execute_input":"2022-05-23T18:44:04.558953Z","iopub.status.idle":"2022-05-23T18:44:05.181097Z","shell.execute_reply.started":"2022-05-23T18:44:04.558910Z","shell.execute_reply":"2022-05-23T18:44:05.180084Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print('Test accuracy = %f' % check_model(model, testloader))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:40:47.253453Z","iopub.execute_input":"2022-05-23T18:40:47.254073Z","iopub.status.idle":"2022-05-23T18:40:59.129793Z","shell.execute_reply.started":"2022-05-23T18:40:47.254033Z","shell.execute_reply":"2022-05-23T18:40:59.128759Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-23T18:56:55.952928Z","iopub.execute_input":"2022-05-23T18:56:55.953277Z","iopub.status.idle":"2022-05-23T18:56:56.089762Z","shell.execute_reply.started":"2022-05-23T18:56:55.953243Z","shell.execute_reply":"2022-05-23T18:56:56.088899Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Dividing the dataset into 5 folds for using the K fold Cross Validation","metadata":{}},{"cell_type":"code","source":"# Configuration options\nk_folds = 5\nepochs = 2\nloss_function = nn.CrossEntropyLoss()\n\n# For fold results\nresults = {}\n\n# Set fixed random number seed\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:16.335288Z","iopub.execute_input":"2022-05-22T11:51:16.335878Z","iopub.status.idle":"2022-05-22T11:51:16.342906Z","shell.execute_reply.started":"2022-05-22T11:51:16.335838Z","shell.execute_reply":"2022-05-22T11:51:16.341168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=k_folds, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:18.275165Z","iopub.execute_input":"2022-05-22T11:51:18.275414Z","iopub.status.idle":"2022-05-22T11:51:18.28033Z","shell.execute_reply.started":"2022-05-22T11:51:18.275383Z","shell.execute_reply":"2022-05-22T11:51:18.2785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {}\nc = 1","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:51:19.353236Z","iopub.execute_input":"2022-05-22T11:51:19.353504Z","iopub.status.idle":"2022-05-22T11:51:19.358316Z","shell.execute_reply.started":"2022-05-22T11:51:19.353473Z","shell.execute_reply":"2022-05-22T11:51:19.357655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-fold Cross Validation model evaluation","metadata":{}},{"cell_type":"code","source":"# K-fold Cross Validation model evaluation\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n\n    # Print\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n\n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n\n    # Define data loaders for training and testing data in this fold\n    trainloader = torch.utils.data.DataLoader(\n                      dataset, \n                      batch_size=10, sampler=train_subsampler)\n    testloader = torch.utils.data.DataLoader(\n                      dataset,\n                      batch_size=10, sampler=test_subsampler)\n\n    # Init the neural network\n    model = ResNet(block, [2,2,2,2], 3, 2)\n\n    # Run the training loop for defined number of epochs\n    losses, accuracies, accuracy = run_epochs(params,model)\n\n    # Process is complete.\n    print('Training process has finished. Saving trained model.')\n    print('--------------------------------')\n    models['model'+str(c)] = model\n    c += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:52:11.360789Z","iopub.execute_input":"2022-05-22T11:52:11.361496Z","iopub.status.idle":"2022-05-22T12:13:08.161428Z","shell.execute_reply.started":"2022-05-22T11:52:11.361452Z","shell.execute_reply":"2022-05-22T12:13:08.160678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:13:08.173924Z","iopub.execute_input":"2022-05-22T12:13:08.17419Z","iopub.status.idle":"2022-05-22T12:13:08.187522Z","shell.execute_reply.started":"2022-05-22T12:13:08.174155Z","shell.execute_reply":"2022-05-22T12:13:08.185914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function for predicting the output using the average of the outputs from the 5 trained models, thus using and applying the ensembling technique","metadata":{}},{"cell_type":"code","source":"def predict(models, data):\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        out = 0\n\n\n        \n        for images,labels in trainloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            prediction = []\n            for i in range(5):\n                outputs = models['model'+str(i+1)](images)\n                _, predicted = torch.max(outputs.data, 1)\n                prediction += predicted \n            \n            prediction = [x/5 for x in prediction]\n            prediction = [(1 if i>0.5 else 0) for i in prediction]\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n    accuracy = (100*correct/total)\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:13:08.163125Z","iopub.execute_input":"2022-05-22T12:13:08.163855Z","iopub.status.idle":"2022-05-22T12:13:08.172591Z","shell.execute_reply.started":"2022-05-22T12:13:08.163815Z","shell.execute_reply":"2022-05-22T12:13:08.171899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = predict(models,dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:04:12.260546Z","iopub.execute_input":"2022-05-22T13:04:12.261216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy = %f' % accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code for saving the models","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    torch.save(models['model'+str(i+1)].state_dict(), f'model{i+1}.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:16:33.381024Z","iopub.execute_input":"2022-05-22T12:16:33.381304Z","iopub.status.idle":"2022-05-22T12:16:33.856817Z","shell.execute_reply.started":"2022-05-22T12:16:33.381273Z","shell.execute_reply":"2022-05-22T12:16:33.856081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code for loading the models","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    torch.load(models['model'+str(i+1)].state_dict(), f'model{i+1}.pth')","metadata":{},"execution_count":null,"outputs":[]}]}